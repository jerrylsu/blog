{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date: 2021-11-23 11:17:17\n",
    "author: Jerry Su\n",
    "slug: Tokenizer-offset-mapping\n",
    "title: Tokenizer offerset mapping\n",
    "category: \n",
    "tags: Paddle, NLP, Pytorch\n",
    "summary: Reason is the light and the light of life.\n",
    "toc: show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = [(0, 0), (0, 1), (1, 2), (2, 3), (3, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (0, 0)]\n",
    "tokens = ['[CLS]', '对', '儿', '童', 'sars', '##t', '细', '胞', '亚', '群', '的', '研', '究', '表', '明', '，', '与', '成', '人', 'sars', '相', '比', '，', '儿', '童', '细', '胞', '下', '降', '不', '明', '显', '，', '证', '明', '上', '述', '推', '测', '成', '立', '。', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(offsets) == len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 (0, 0) [CLS] 0 -1\n",
      "0 (0, 1) 对 0 0\n",
      "1 (1, 2) 儿 1 1\n",
      "2 (2, 3) 童 2 2\n",
      "3 (3, 7) sars 3 6\n",
      "4 (7, 8) ##t 7 7\n",
      "5 (8, 9) 细 8 8\n",
      "6 (9, 10) 胞 9 9\n",
      "7 (10, 11) 亚 10 10\n",
      "8 (11, 12) 群 11 11\n",
      "9 (12, 13) 的 12 12\n",
      "10 (13, 14) 研 13 13\n",
      "11 (14, 15) 究 14 14\n",
      "12 (15, 16) 表 15 15\n",
      "13 (16, 17) 明 16 16\n",
      "14 (17, 18) ， 17 17\n",
      "15 (18, 19) 与 18 18\n",
      "16 (19, 20) 成 19 19\n",
      "17 (20, 21) 人 20 20\n",
      "18 (21, 25) sars 21 24\n",
      "19 (25, 26) 相 25 25\n",
      "20 (26, 27) 比 26 26\n",
      "21 (27, 28) ， 27 27\n",
      "22 (28, 29) 儿 28 28\n",
      "23 (29, 30) 童 29 29\n",
      "24 (30, 31) 细 30 30\n",
      "25 (31, 32) 胞 31 31\n",
      "26 (32, 33) 下 32 32\n",
      "27 (33, 34) 降 33 33\n",
      "28 (34, 35) 不 34 34\n",
      "29 (35, 36) 明 35 35\n",
      "30 (36, 37) 显 36 36\n",
      "31 (37, 38) ， 37 37\n",
      "32 (38, 39) 证 38 38\n",
      "33 (39, 40) 明 39 39\n",
      "34 (40, 41) 上 40 40\n",
      "35 (41, 42) 述 41 41\n",
      "36 (42, 43) 推 42 42\n",
      "37 (43, 44) 测 43 43\n",
      "38 (44, 45) 成 44 44\n",
      "39 (45, 46) 立 45 45\n",
      "40 (46, 47) 。 46 46\n",
      "41 (0, 0) [SEP] 0 -1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "offset: (start, end)\n",
    "\n",
    "start = text.index(token) token在原文本的\n",
    "\n",
    "end = start + len(tokens)   # len('##') = 0 means except \"##\"\n",
    "\"\"\"\n",
    "\n",
    "for idx, (offset, token) in enumerate(zip(offsets, tokens)):\n",
    "    print(idx - 1, offset, token, offset[0], offset[-1] - 1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

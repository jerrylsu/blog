{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date: 2021-08-20 10:17:17\n",
    "author: Jerry Su\n",
    "slug: NLP-Unicode-Tokenizer-Mapping\n",
    "title: NLP Unicode Tokenizer Mapping\n",
    "category: \n",
    "tags: Python, NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils.py\n",
    "# tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1.0'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.unidata_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Po\n",
      "Pd\n",
      "Po\n",
      "Zs\n",
      "Ll\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n"
     ]
    }
   ],
   "source": [
    "# unicode类型枚举\n",
    "# https://www.fileformat.info/info/unicode/category/index.htm\n",
    "\n",
    "print(unicodedata.category('.'))\n",
    "print(unicodedata.category('-'))\n",
    "print(unicodedata.category(','))\n",
    "print(unicodedata.category(' '))\n",
    "#unicodedata.category('a๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎')\n",
    "for ch in 'a๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎':\n",
    "    print(unicodedata.category(ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "10\n",
      "13\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# http://c.biancheng.net/c/ascii/\n",
    "# ord()返回十进制\n",
    "print(ord(' '))\n",
    "print(ord('\\n'))\n",
    "print(ord('\\r'))\n",
    "print(ord('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 判断空格\n",
    "def is_space(ch):\n",
    "    \"\"\"空格类字符判断判断。\n",
    "    \n",
    "    空格字符包括：' ', '\\n', '\\t', 'r'\n",
    "    \"\"\"\n",
    "    return ch == ' ' or \\\n",
    "           ch == '\\t' or \\\n",
    "           ch == '\\r' or \\\n",
    "           ch == '\\n' or \\\n",
    "           unicodedata.category(ch) == 'Zs'  # [Zs] Separator, Space\n",
    "\n",
    "print(is_space(' '))\n",
    "print(is_space('\\n'))\n",
    "print(is_space('\\r'))\n",
    "print(is_space('\\t'))\n",
    "print(is_space('A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sm'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断标点符号\n",
    "def is_punctuation(ch):\n",
    "    \"\"\"标点符号类字符判断（包含全/半角）。\n",
    "    英文标点符号是半角，中文标点符号是全角。全角占一个字符，半角占两个字符。\n",
    "    \n",
    "    [33, 47]    ! \" # $ % & ' ( ) * + , - . /\n",
    "    [58, 64]    : ; < = > ? @\n",
    "    [91, 96]    [ \\ ] ^ _ `\n",
    "    [123, 126]  { | } ~\n",
    "    \n",
    "    unicodedata.category.(ch).startswith('P')\n",
    "    # https://www.fileformat.info/info/unicode/category/index.htm]\n",
    "    # 包含了所有标点符号[Pc][Pd][Pe][Pf][Pi][Po][Ps]\n",
    "    \"\"\"\n",
    "    code = ord(ch)\n",
    "    return 33 <= code <= 47 or \\\n",
    "           58 <= code <= 64 or \\\n",
    "           91 <= code <= 96 or \\\n",
    "           123 <= code <= 126 or \\\n",
    "           unicodedata.category(ch).startswith('P')\n",
    "is_punctuation('？') # 中文？号\n",
    "unicodedata.category('∫')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_control(ch):\n",
    "    \"\"\"控制类字符判断\n",
    "    https://en.wikipedia.org/wiki/Control_character\n",
    "    https://www.fileformat.info/info/unicode/category/Cc/index.htm\n",
    "    https://www.fileformat.info/info/unicode/category/Cf/index.htm\n",
    "    \n",
    "    \"\"\"\n",
    "    return unicodedata.category(ch) in ('Cc', 'Cf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cjk_character(ch):\n",
    "    \"\"\"CJK类字符判断（包括中文字符也在此列）\n",
    "    参考：https://en.wikipedia.org/wiki/Unicode_block\n",
    "    \n",
    "    # CJK Unified Ideographs, HAN\n",
    "    # CJK Unified Ideographs Extension A, HAN\n",
    "    # General Punctuation\n",
    "    # Supplemental Mathematical Operators\n",
    "    # Miscellaneous Symbols and Arrows\n",
    "    # Miscellaneous Symbols and Arrows\n",
    "    # CJK Compatibility Ideographs, HAN\n",
    "    # CJK Compatibility Ideographs Supplement, HAN\n",
    "    \"\"\"\n",
    "    code = ord(ch)\n",
    "    return 0x4E00 <= code <= 0x9FFF or \\\n",
    "           0x3400 <= code <= 0x4DBF or \\\n",
    "           0x20000 <= code <= 0x2A6DF or \\\n",
    "           0x2A700 <= code <= 0x2B73F or \\\n",
    "           0x2B740 <= code <= 0x2B81F or \\\n",
    "           0x2B820 <= code <= 0x2CEAF or \\\n",
    "           0xF900 <= code <= 0xFAFF or \\\n",
    "           0x2F800 <= code <= 0x2FA1F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin: a๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎b是特殊mn字符123.4! 56~ jerry！ 数学符号🤌中国ustc : 57\n",
      "Normalize: a๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎b是特殊mn字符123.4! 56~ jerry！ 数学符号🤌中国ustc : 57\n",
      "Text: ab是特殊mn字符123.4! 56~ jerry！ 数学符号🤌中国ustc : 38\n",
      "Spaced text: ab 是  特  殊 mn 字  符 123 . 4 !  56 ~  jerry ！   数  学  符  号 🤌 中  国 ustc\n"
     ]
    }
   ],
   "source": [
    "\"\"\"分词器\n",
    "\"\"\"\n",
    "\n",
    "origin_text = \"a๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎๎b是特殊Mn字符123.4! 56~ jerry！ 数学符号🤌中国USTC\"\n",
    "text = origin_text.lower()\n",
    "print(f'Origin: {text} : {len(text)}')\n",
    "\n",
    "# 由于存在Mn字符等一些特殊字符，先规范化normalize，NFD规范分解形式\n",
    "text = unicodedata.normalize('NFD', text)\n",
    "print(f'Normalize: {text} : {len(text)}')\n",
    "\n",
    "# 删除Mn字符\n",
    "text = ''.join([ch for ch in text if unicodedata.category(ch) != 'Mn'])\n",
    "print(f'Text: {text} : {len(text)}')\n",
    "\n",
    "\n",
    "# 空格分隔文本\n",
    "spaced = ''\n",
    "for ch in text:\n",
    "    # 标点符合和cjk字符，前后空格分隔\n",
    "    if is_punctuation(ch) or is_cjk_character(ch):\n",
    "        spaced += ' ' + ch + ' '\n",
    "    # 空格，即置空格\n",
    "    elif is_space(ch):   \n",
    "        spaced += ' '\n",
    "    # 删除0(NULL)，0xfffd，控制字符，这些字符均是不可见，无法显示的\n",
    "    elif ord(ch) == 0 or ord(ch) == 0xfffd or is_control(ch):  \n",
    "        continue\n",
    "    # 数字/英文字母/数学符号，直接拼接\n",
    "    else:    \n",
    "        spaced += ch\n",
    "print(f'Spaced text: {spaced}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-23 02:00:28--  https://huggingface.co/hfl/chinese-roberta-wwm-ext/blob/main/vocab.txt\n",
      "Resolving huggingface.co (huggingface.co)... 34.200.164.230, 54.84.221.171, 34.195.144.223, ...\n",
      "Connecting to huggingface.co (huggingface.co)|34.200.164.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4564022 (4.4M) [text/html]\n",
      "Saving to: ‘vocab.txt’\n",
      "\n",
      "vocab.txt           100%[===================>]   4.35M  2.58MB/s    in 1.7s    \n",
      "\n",
      "2021-08-23 02:00:32 (2.58 MB/s) - ‘vocab.txt’ saved [4564022/4564022]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/hfl/chinese-roberta-wwm-ext/blob/main/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path):\n",
    "    tokens_dict = {}\n",
    "    with open(vocab_path, encoding='utf-8') as fp:\n",
    "        for line in fp:\n",
    "            token = line.split()\n",
    "            token = token[0] if token else line.strip()\n",
    "            tokens_dict[token] = len(tokens_dict)\n",
    "    return tokens_dict\n",
    "tokens_dict = load_vocab('../../../cache/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_piece_tokenize(word):\n",
    "    \"\"\"word内分成subword, _word_maxlen=200\n",
    "    \"\"\"\n",
    "    if len(word) > 200:\n",
    "        return [word]\n",
    "\n",
    "    tokens, start, end = [], 0, 0\n",
    "    while start < len(word):\n",
    "        end = len(word)\n",
    "        while end > start:\n",
    "            sub = word[start:end]\n",
    "            if start > 0:\n",
    "                sub = '##' + sub\n",
    "            if sub in tokens_dict:\n",
    "                break\n",
    "            end -= 1\n",
    "        if start == end:\n",
    "            return [word]\n",
    "        else:\n",
    "            tokens.append(sub)\n",
    "            start = end\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\n",
      "是\n",
      "特\n",
      "殊\n",
      "mn\n",
      "字\n",
      "符\n",
      "123\n",
      ".\n",
      "4\n",
      "!\n",
      "56\n",
      "~\n",
      "jerry\n",
      "！\n",
      "数\n",
      "学\n",
      "符\n",
      "号\n",
      "🤌\n",
      "中\n",
      "国\n",
      "ustc\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for word in spaced.strip().split():\n",
    "    print(word)\n",
    "    tokens.extend(word_piece_tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Rematch算法\n",
    "解决原文本context中answer_start_id，到tokenizer化以后在tokens列表中的index映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = [[], [0], [1], [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def rematch(text, tokens):\n",
    "        \"\"\"给出原始的text和tokenize后的tokens的映射关系\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "\n",
    "        normalized_text, char_mapping = '', []\n",
    "        for i, ch in enumerate(text):\n",
    "            if True:\n",
    "                ch = unicodedata.normalize('NFD', ch)\n",
    "                ch = ''.join([c for c in ch if unicodedata.category(c) != 'Mn'])\n",
    "            ch = ''.join([\n",
    "                c for c in ch\n",
    "                if not (ord(c) == 0 or ord(c) == 0xfffd or is_control(c))\n",
    "            ])\n",
    "            normalized_text += ch\n",
    "            char_mapping.extend([i] * len(ch))\n",
    "\n",
    "        text, token_mapping, offset = normalized_text, [], 0\n",
    "        print(text)\n",
    "        for token in tokens:\n",
    "            start = text[offset:].index(token) + offset\n",
    "            end = start + len(token)\n",
    "            token_mapping.append(char_mapping[start:end])\n",
    "            offset = end\n",
    "        return token_mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:blog] *",
   "language": "python",
   "name": "conda-env-blog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- date: 2022-11-03 11:17:17\n",
    "- author: Jerry Su\n",
    "- slug: Paddle-Adversarial-Train\n",
    "- title: Paddle adversarial train\n",
    "- category: \n",
    "- tags: Paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGM(object):\n",
    "    \"\"\"Fast Gradient Method.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, eps=1.):\n",
    "        self.model = (model.module if hasattr(model, \"module\") else model)\n",
    "        self.eps = eps\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, emb_name='embedding'):\n",
    "        \"\"\"only attack embedding\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.stop_gradient and emb_name in name:\n",
    "                self.backup[name] = param.clone()        # 备份\n",
    "                norm = paddle.norm(param.grad)           # embedding梯度\n",
    "                if norm and not paddle.isnan(norm):\n",
    "                    r_at = self.eps * param.grad / norm  # 计算扰动\n",
    "                    param.add(r_at)                      # 注入扰动\n",
    "\n",
    "    def restore(self, emb_name='embedding'):\n",
    "        \"\"\"Restore model parameter to correct position; \n",
    "        FGM do not perturbe weights, it perturb gradients;\n",
    "        Call after loss.backward(), before optimizer.step()\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.stop_gradient and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.set_value(self.backup[name])      # 删除扰动\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP(object):\n",
    "    \"\"\"Adversarial Weight Perturbation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer=None,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=1,\n",
    "        adv_eps=0.001,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def perturb(self):\n",
    "        \"\"\"Perturb model parameters for AWP gradient\n",
    "        Call before loss and loss.backward()\n",
    "        \"\"\"\n",
    "        if (self.adv_lr == 0):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        self._attack_step()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if (not param.stop_gradient) and (param.grad is not None) and (self.adv_param in name):\n",
    "                norm1 = paddle.norm(param.grad)\n",
    "                norm2 = paddle.norm(param.detach())\n",
    "                if norm1 != 0 and not paddle.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.add(r_at)\n",
    "                    param = paddle.min(\n",
    "                        paddle.max(param, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if (not param.stop_gradient) and (param.grad is not None) and (self.adv_param in name):\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def restore(self,):\n",
    "        \"\"\"Restore model parameter to correct position; \n",
    "        AWP do not perturbe weights, it perturb gradients;\n",
    "        Call after loss.backward(), before optimizer.step()\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.set_value(self.backup[name])\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "fgm = FGM(model=model) if args.do_adversarial_train else None\n",
    "self.adv = fgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGM adversarial train\n",
    "if self.adv and self.state.epoch > self.args.num_train_epochs // 2:\n",
    "    self.adv.attack()\n",
    "    # FGM\n",
    "    with self.autocast_smart_context_manager():\n",
    "        loss_adv = self.compute_loss(model, inputs, labels)\n",
    "    if self.args.gradient_accumulation_steps > 1:\n",
    "        loss_adv = loss_adv / self.args.gradient_accumulation_steps\n",
    "    loss_adv.backward()\n",
    "    self.adv.restore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:blog]",
   "language": "python",
   "name": "conda-env-blog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

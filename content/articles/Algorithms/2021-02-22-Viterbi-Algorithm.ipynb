{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- date: 2021-02-22 10:17:17\n",
    "- author: Jerry Su\n",
    "- slug: Viterbi-Algorithm\n",
    "- title: Viterbi Algorithm\n",
    "- category: \n",
    "- tags: Viterbi, Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵法numpy\n",
    "\n",
    "注：转移矩阵和发射矩阵以列维度上形式表示概率分布的，即列维度axis=1上元素和为1。**注意：把隐状态概率分布表示为向量后，向量乘以状态转移矩阵的结果为新的隐状态。** 同理，对发射矩阵做向量矩阵乘法会得到新的观测状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(score_prev: np.ndarray,\n",
    "         emission_probs: np.ndarray,\n",
    "         transition_probs: np.ndarray,\n",
    "         observed_state: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"运行维特比算法一个时间步。\n",
    "    \n",
    "    Args:\n",
    "        score_prev: probability distribution with shape (num_hidden),\n",
    "            the previous score\n",
    "        emission_probs: the emission probability matrix (num_hidden,\n",
    "            num_observed)\n",
    "        transition_probs: the transition probability matrix, with\n",
    "            shape (num_hidden, num_hidden)\n",
    "        observed_state: the observed state at the current step\n",
    "    \n",
    "    Returns:\n",
    "        - the score for the next step\n",
    "        - the maximizing previous state, before the current state, as an int array with shape (num_hidden)\n",
    "    \"\"\"\n",
    "    pre_max = score_prev * transition_probs.T                             # 乘以各个隐状态转移概率下的分数值（矩阵乘法并非点积，向量广播成矩阵后对应元素相乘）\n",
    "    max_prev_states = np.argmax(pre_max, axis=1)                          # 来自前一时间步的哪个隐状态。即对应前一时间步隐状态位置\n",
    "    max_vals = pre_max[np.arange(len(max_prev_states)), max_prev_states]  # 根据最大值索引取出最大值\n",
    "    score_new = max_vals * emission_probs[:, observed_state]                 # 发射概率列向量：该观测状态对应的隐状态列\n",
    "    \n",
    "    return score_new, max_prev_states\n",
    "\n",
    "def viterbi(emission_probs: np.ndarray,\n",
    "            transition_probs: np.array,\n",
    "            start_probs: np.ndarray,\n",
    "            observed_states: List[int]) -> Tuple[List[int], float]:\n",
    "    \"\"\"运行维特比算法获得最有可能的状态序列。\n",
    "    \n",
    "    Args:\n",
    "        emission_probs:\n",
    "        transition_probs:\n",
    "        start_probs:\n",
    "        observed_states:\n",
    "        \n",
    "    Returns:\n",
    "        - 最有可能的状态序列。\n",
    "        - 状态和观测序列的联合概率值。\n",
    "    \"\"\"\n",
    "    # 运行正向传递，存储最有可能的先前状态。\n",
    "    score = start_probs * emission_probs[:, observed_states[0]]  # 第一个时间步的最优解向量（隐含状态向量维度）\n",
    "    all_pre_states = []\n",
    "    for observed_state in observed_states[1:]:\n",
    "        score, prevs = step(score, emission_probs, transition_probs, observed_state)\n",
    "        all_pre_states.append(prevs)\n",
    "        \n",
    "    # 回溯\n",
    "    state = np.argmax(score)            # 最后一个时间步最大分数出现的位置，该值表示来自前一个时间步的第几个状态\n",
    "    sequence_score = score[state]       # 获取最大分数，即累计最大分数，最大隐状态序列\n",
    "    sequence_state = [state]            # 最后一个时间步的最优隐状态加入结果序列\n",
    "    for pre_states in all_pre_states[::-1]:\n",
    "        state = pre_states[state]       # 回溯获取前一个最优隐状态位置\n",
    "        sequence_state.append(state)    # 加入结果序列\n",
    "    return sequence_state[::-1], sequence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_states = 3     # vocab_size\n",
    "num_observed_states = 2   # vocab_size\n",
    "num_time_steps = 4        # sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化转移概率矩阵\n",
    "transition_probs = np.array([\n",
    "    [0.1, 0.2, 0.7],\n",
    "    [0.1, 0.1, 0.8],\n",
    "    [0.5, 0.4, 0.1],\n",
    "])\n",
    "assert transition_probs.shape == (num_hidden_states, num_hidden_states)\n",
    "assert transition_probs.sum(axis=1).mean() == 1.0\n",
    "\n",
    "# 初始化发射概率矩阵 （行 -> 隐含状态：0，1，2，列 -> 观测状态：0，1）\n",
    "emission_probs = np.array([\n",
    "    [0.1, 0.9],\n",
    "    [0.3, 0.7],\n",
    "    [0.5, 0.5],    \n",
    "])\n",
    "assert emission_probs.shape == (num_hidden_states, num_observed_states)\n",
    "assert emission_probs.sum(axis=1).mean() == 1.0\n",
    "\n",
    "# 初始隐状态概率\n",
    "init_hidden_state_probs = np.array([0.1, 0.3, 0.6])\n",
    "assert init_hidden_state_probs.shape == (num_hidden_states,)\n",
    "\n",
    "# 定义观测序列\n",
    "observed_states = [1, 1, 0, 1]\n",
    "assert len(observed_states) == num_time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 0, 2, 0], 0.0212625)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq, seq_prob = viterbi(\n",
    "    emission_probs,\n",
    "    transition_probs,\n",
    "    init_hidden_state_probs,\n",
    "    observed_states,\n",
    ")\n",
    "max_seq, seq_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.3, 0.6])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_hidden_state_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.9],\n",
       "       [0.3, 0.7],\n",
       "       [0.5, 0.5]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.09, 0.3 ])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = init_hidden_state_probs * emission_probs[:, 0]\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1, 0.5],\n",
       "       [0.2, 0.1, 0.4],\n",
       "       [0.7, 0.8, 0.1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_probs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001, 0.009, 0.15 ],\n",
       "       [0.002, 0.009, 0.12 ],\n",
       "       [0.007, 0.072, 0.03 ]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_max = score * transition_probs.T\n",
    "pre_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_prev_states = np.argmax(pre_max, axis=1)\n",
    "max_prev_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(max_prev_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15 , 0.12 , 0.072])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_max[np.arange(len(max_prev_states)), max_prev_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.135, 0.084, 0.036]), array([2, 2, 1]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(score, emission_probs, transition_probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵法pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: (1, 5, 3)\n",
    "logits = torch.rand((1,5,3))\n",
    "mask = torch.tensor([[1,1,1,0,0]])\n",
    "transisition_probs = torch.rand((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, max_len, n_tags = logits.size()\n",
    "seq_len = mask.long().sum(dim=1)\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "logits = logits.transpose(0,1).data\n",
    "print(logits.shape)\n",
    "mask = mask.transpose(0,1).data.eq(True)\n",
    "print(mask.shape)\n",
    "flip_mask = mask.eq(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 3])\n",
      "tensor([[0.3556, 0.4721, 0.6929]])\n"
     ]
    }
   ],
   "source": [
    "vpath = torch.zeros_like(logits, dtype=torch.int)\n",
    "print(vpath.shape)\n",
    "vscore = logits[0]  # 初始化\n",
    "print(vscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.]]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_transition_prob = torch.zeros(3).view(1, 3).repeat(batch_size,1,1)\n",
    "end_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def viterbi_decode(self, logits, mask, unpad=False):\n",
    "        r\"\"\"给定一个特征矩阵以及转移分数矩阵，计算出最佳的路径以及对应的分数\n",
    "\n",
    "        :param torch.FloatTensor logits: batch_size x max_len x num_tags，特征矩阵。\n",
    "        :param torch.ByteTensor mask: batch_size x max_len, 为0的位置认为是pad；如果为None，则认为没有padding。\n",
    "        :param bool unpad: 是否将结果删去padding。False, 返回的是batch_size x max_len的tensor; True，返回的是\n",
    "            List[List[int]], 内部的List[int]为每个sequence的label，已经除去pad部分，即每个List[int]的长度是这\n",
    "            个sample的有效长度。\n",
    "        :return: 返回 (paths, scores)。\n",
    "                    paths: 是解码后的路径, 其值参照unpad参数.\n",
    "                    scores: torch.FloatTensor, size为(batch_size,), 对应每个最优路径的分数。\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size, max_len, n_tags = logits.size()\n",
    "        seq_len = mask.long().sum(1)\n",
    "        logits = logits.transpose(0, 1).data  # L, B, H\n",
    "        mask = mask.transpose(0, 1).data.eq(True)  # L, B\n",
    "        flip_mask = mask.eq(False)\n",
    "\n",
    "        # dp\n",
    "        vpath = logits.new_zeros((max_len, batch_size, n_tags), dtype=torch.long)  # torch.zeros_like(logits, dtype=torch.int)\n",
    "        vscore = logits[0]  # bsz x n_tags         # 初始化\n",
    "        transitions = self._constrain.data.clone()\n",
    "        transitions[:n_tags, :n_tags] += self.trans_m.data\n",
    "        if self.include_start_end_trans:\n",
    "            transitions[n_tags, :n_tags] += self.start_scores.data\n",
    "            transitions[:n_tags, n_tags + 1] += self.end_scores.data\n",
    "\n",
    "        vscore += transitions[n_tags, :n_tags]   # add all 0\n",
    "\n",
    "        trans_score = transitions[:n_tags, :n_tags].view(1, n_tags, n_tags).data  # 转移概率矩阵\n",
    "        end_trans_score = transitions[:n_tags, n_tags + 1].view(1, 1, n_tags).repeat(batch_size, 1, 1)  # bsz, 1, n_tags\n",
    "\n",
    "        # 针对长度为1的句子, 长度非1的句子则作为初始值\n",
    "        vscore += transitions[:n_tags, n_tags + 1].view(1, n_tags).repeat(batch_size, 1).masked_fill(seq_len.ne(1).view(-1, 1), 0)\n",
    "\n",
    "        for i in range(1, max_len):\n",
    "            prev_score = vscore.view(batch_size, n_tags, 1)\n",
    "            cur_score = logits[i].view(batch_size, 1, n_tags) + trans_score    # emission_prob: logits (1,1,8) + (1,8,8)\n",
    "            score = prev_score + cur_score.masked_fill(flip_mask[i].view(batch_size, 1, 1), 0)  # bsz x n_tag x n_tag\n",
    "            # 需要考虑当前位置是该序列的最后一个\n",
    "            score += end_trans_score.masked_fill(seq_len.ne(i + 1).view(-1, 1, 1), 0)\n",
    "\n",
    "            best_score, best_dst = score.max(1)\n",
    "            vpath[i] = best_dst\n",
    "            # 由于最终是通过last_tags回溯，需要保持每个位置的vscore情况\n",
    "            vscore = best_score.masked_fill(flip_mask[i].view(batch_size, 1), 0) + vscore.masked_fill(mask[i].view(batch_size, 1), 0)\n",
    "        # 上面masked_fill均是解决mask为0情况\n",
    "\n",
    "        # backtrace\n",
    "        batch_idx = torch.arange(batch_size, dtype=torch.long, device=logits.device)\n",
    "        seq_idx = torch.arange(max_len, dtype=torch.long, device=logits.device)\n",
    "        lens = (seq_len - 1)\n",
    "        # idxes [L, B], batched idx from seq_len-1 to 0\n",
    "        idxes = (lens.view(1, -1) - seq_idx.view(-1, 1)) % max_len\n",
    "\n",
    "        ans = logits.new_empty((max_len, batch_size), dtype=torch.long)\n",
    "        ans_score, last_tags = vscore.max(1)  # 最优路径分数，和最后一个时间步的最优隐状态\n",
    "        ans[idxes[0], batch_idx] = last_tags\n",
    "        for i in range(max_len - 1):\n",
    "            last_tags = vpath[idxes[i], batch_idx, last_tags]\n",
    "            ans[idxes[i + 1], batch_idx] = last_tags\n",
    "        ans = ans.transpose(0, 1)\n",
    "        if unpad:\n",
    "            paths = []\n",
    "            for idx, max_len in enumerate(lens):\n",
    "                paths.append(ans[idx, :max_len + 1].tolist())\n",
    "        else:\n",
    "            paths = ans\n",
    "        return paths, ans_scor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001, 0.002, 0.007, 0.009, 0.009, 0.072, 0.15 , 0.12 , 0.03 ]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001, 0.002, 0.007],\n",
       "       [0.009, 0.009, 0.072],\n",
       "       [0.15 , 0.12 , 0.03 ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pre_max.reshape(3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0,0]=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pre_max.reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001, 0.002, 0.007],\n",
       "       [0.009, 0.009, 0.072],\n",
       "       [0.15 , 0.12 , 0.03 ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = b.resize(1,9)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001, 0.002, 0.007, 0.009, 0.009, 0.072, 0.15 , 0.12 , 0.03 ]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 2021-02-22-Viterbi-Algorithm.ipynb to markdown\n",
      "[NbConvertApp] Writing 9793 bytes to 2021-02-22-Viterbi-Algorithm.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to markdown 2021-02-22-Viterbi-Algorithm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

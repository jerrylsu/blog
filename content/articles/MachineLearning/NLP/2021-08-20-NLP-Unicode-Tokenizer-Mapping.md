date: 2021-08-20 10:17:17
author: Jerry Su
slug: NLP-Unicode-Tokenizer-Mapping
title: NLP Unicode Tokenizer Mapping
category: 
tags: Python, NLP


```python
# https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils.py
# tokenzier
```


```python
# http://www.unicode.org/reports/tr44/#GC_Values_Table
import unicodedata
```


```python
unicodedata.unidata_version
```




    '12.1.0'




```python
# unicodeç±»å‹æšä¸¾
# https://www.fileformat.info/info/unicode/category/index.htm

print(unicodedata.category('.'))
print(unicodedata.category('-'))
print(unicodedata.category(','))
print(unicodedata.category(' '))
#unicodedata.category('aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹')
for ch in 'aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹':
    print(unicodedata.category(ch))
```

    Po
    Pd
    Po
    Zs
    Ll
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn
    Mn



```python
# http://c.biancheng.net/c/ascii/
# ord()è¿”å›åè¿›åˆ¶
print(ord(' '))
print(ord('\n'))
print(ord('\r'))
print(ord('\t'))
```

    32
    10
    13
    9



```python
# åˆ¤æ–­ç©ºæ ¼
def is_space(ch):
    """ç©ºæ ¼ç±»å­—ç¬¦åˆ¤æ–­åˆ¤æ–­ã€‚
    
    ç©ºæ ¼å­—ç¬¦åŒ…æ‹¬ï¼š' ', '\n', '\t', 'r'
    """
    return ch == ' ' or \
           ch == '\t' or \
           ch == '\r' or \
           ch == '\n' or \
           unicodedata.category(ch) == 'Zs'  # [Zs] Separator, Space

print(is_space(' '))
print(is_space('\n'))
print(is_space('\r'))
print(is_space('\t'))
print(is_space('A'))
```

    True
    True
    True
    True
    False



```python
# åˆ¤æ–­æ ‡ç‚¹ç¬¦å·
def is_punctuation(ch):
    """æ ‡ç‚¹ç¬¦å·ç±»å­—ç¬¦åˆ¤æ–­ï¼ˆåŒ…å«å…¨/åŠè§’ï¼‰ã€‚
    è‹±æ–‡æ ‡ç‚¹ç¬¦å·æ˜¯åŠè§’ï¼Œä¸­æ–‡æ ‡ç‚¹ç¬¦å·æ˜¯å…¨è§’ã€‚å…¨è§’å ä¸€ä¸ªå­—ç¬¦ï¼ŒåŠè§’å ä¸¤ä¸ªå­—ç¬¦ã€‚
    
    [33, 47]    ! " # $ % & ' ( ) * + , - . /
    [58, 64]    : ; < = > ? @
    [91, 96]    [ \ ] ^ _ `
    [123, 126]  { | } ~
    
    unicodedata.category.(ch).startswith('P')
    # https://www.fileformat.info/info/unicode/category/index.htm]
    # åŒ…å«äº†æ‰€æœ‰æ ‡ç‚¹ç¬¦å·[Pc][Pd][Pe][Pf][Pi][Po][Ps]
    """
    code = ord(ch)
    return 33 <= code <= 47 or \
           58 <= code <= 64 or \
           91 <= code <= 96 or \
           123 <= code <= 126 or \
           unicodedata.category(ch).startswith('P')
is_punctuation('ï¼Ÿ') # ä¸­æ–‡ï¼Ÿå·
unicodedata.category('âˆ«')
```




    'Sm'




```python
def is_control(ch):
    """æ§åˆ¶ç±»å­—ç¬¦åˆ¤æ–­
    https://en.wikipedia.org/wiki/Control_character
    https://www.fileformat.info/info/unicode/category/Cc/index.htm
    https://www.fileformat.info/info/unicode/category/Cf/index.htm
    
    """
    return unicodedata.category(ch) in ('Cc', 'Cf')
```


```python
def is_cjk_character(ch):
    """CJKç±»å­—ç¬¦åˆ¤æ–­ï¼ˆåŒ…æ‹¬ä¸­æ–‡å­—ç¬¦ä¹Ÿåœ¨æ­¤åˆ—ï¼‰
    å‚è€ƒï¼šhttps://en.wikipedia.org/wiki/Unicode_block
    
    # CJK Unified Ideographs, HAN
    # CJK Unified Ideographs Extension A, HAN
    # General Punctuation
    # Supplemental Mathematical Operators
    # Miscellaneous Symbols and Arrows
    # Miscellaneous Symbols and Arrows
    # CJK Compatibility Ideographs, HAN
    # CJK Compatibility Ideographs Supplement, HAN
    """
    code = ord(ch)
    return 0x4E00 <= code <= 0x9FFF or \
           0x3400 <= code <= 0x4DBF or \
           0x20000 <= code <= 0x2A6DF or \
           0x2A700 <= code <= 0x2B73F or \
           0x2B740 <= code <= 0x2B81F or \
           0x2B820 <= code <= 0x2CEAF or \
           0xF900 <= code <= 0xFAFF or \
           0x2F800 <= code <= 0x2FA1F
```


```python
"""åˆ†è¯å™¨
"""

origin_text = "aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹bæ˜¯ç‰¹æ®ŠMnå­—ç¬¦123.4! 56~ jerryï¼ æ•°å­¦ç¬¦å·ğŸ¤Œä¸­å›½USTC"
text = origin_text.lower()
print(f'Origin: {text} : {len(text)}')

# ç”±äºå­˜åœ¨Mnå­—ç¬¦ç­‰ä¸€äº›ç‰¹æ®Šå­—ç¬¦ï¼Œå…ˆè§„èŒƒåŒ–normalizeï¼ŒNFDè§„èŒƒåˆ†è§£å½¢å¼
text = unicodedata.normalize('NFD', text)
print(f'Normalize: {text} : {len(text)}')

# åˆ é™¤Mnå­—ç¬¦
text = ''.join([ch for ch in text if unicodedata.category(ch) != 'Mn'])
print(f'Text: {text} : {len(text)}')


# ç©ºæ ¼åˆ†éš”æ–‡æœ¬
spaced = ''
for ch in text:
    # æ ‡ç‚¹ç¬¦åˆå’Œcjkå­—ç¬¦ï¼Œå‰åç©ºæ ¼åˆ†éš”
    if is_punctuation(ch) or is_cjk_character(ch):
        spaced += ' ' + ch + ' '
    # ç©ºæ ¼ï¼Œå³ç½®ç©ºæ ¼
    elif is_space(ch):   
        spaced += ' '
    # åˆ é™¤0(NULL)ï¼Œ0xfffdï¼Œæ§åˆ¶å­—ç¬¦ï¼Œè¿™äº›å­—ç¬¦å‡æ˜¯ä¸å¯è§ï¼Œæ— æ³•æ˜¾ç¤ºçš„
    elif ord(ch) == 0 or ord(ch) == 0xfffd or is_control(ch):  
        continue
    # æ•°å­—/è‹±æ–‡å­—æ¯/æ•°å­¦ç¬¦å·ï¼Œç›´æ¥æ‹¼æ¥
    else:    
        spaced += ch
print(f'Spaced text: {spaced}')
```

    Origin: aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹bæ˜¯ç‰¹æ®Šmnå­—ç¬¦123.4! 56~ jerryï¼ æ•°å­¦ç¬¦å·ğŸ¤Œä¸­å›½ustc : 57
    Normalize: aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹bæ˜¯ç‰¹æ®Šmnå­—ç¬¦123.4! 56~ jerryï¼ æ•°å­¦ç¬¦å·ğŸ¤Œä¸­å›½ustc : 57
    Text: abæ˜¯ç‰¹æ®Šmnå­—ç¬¦123.4! 56~ jerryï¼ æ•°å­¦ç¬¦å·ğŸ¤Œä¸­å›½ustc : 38
    Spaced text: ab æ˜¯  ç‰¹  æ®Š mn å­—  ç¬¦ 123 . 4 !  56 ~  jerry ï¼   æ•°  å­¦  ç¬¦  å· ğŸ¤Œ ä¸­  å›½ ustc



```python
!wget https://huggingface.co/hfl/chinese-roberta-wwm-ext/blob/main/vocab.txt
```

    --2021-08-23 02:00:28--  https://huggingface.co/hfl/chinese-roberta-wwm-ext/blob/main/vocab.txt
    Resolving huggingface.co (huggingface.co)... 34.200.164.230, 54.84.221.171, 34.195.144.223, ...
    Connecting to huggingface.co (huggingface.co)|34.200.164.230|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 4564022 (4.4M) [text/html]
    Saving to: â€˜vocab.txtâ€™
    
    vocab.txt           100%[===================>]   4.35M  2.58MB/s    in 1.7s    
    
    2021-08-23 02:00:32 (2.58 MB/s) - â€˜vocab.txtâ€™ saved [4564022/4564022]
    



```python
def load_vocab(vocab_path):
    tokens_dict = {}
    with open(vocab_path, encoding='utf-8') as fp:
        for line in fp:
            token = line.split()
            token = token[0] if token else line.strip()
            tokens_dict[token] = len(tokens_dict)
    return tokens_dict
tokens_dict = load_vocab('../../../cache/vocab.txt')
```


```python
def word_piece_tokenize(word):
    """wordå†…åˆ†æˆsubword, _word_maxlen=200
    """
    if len(word) > 200:
        return [word]

    tokens, start, end = [], 0, 0
    while start < len(word):
        end = len(word)
        while end > start:
            sub = word[start:end]
            if start > 0:
                sub = '##' + sub
            if sub in tokens_dict:
                break
            end -= 1
        if start == end:
            return [word]
        else:
            tokens.append(sub)
            start = end
    return tokens
```


```python
tokens = []
for word in spaced.strip().split():
    print(word)
    tokens.extend(word_piece_tokenize(word))
```

    ab
    æ˜¯
    ç‰¹
    æ®Š
    mn
    å­—
    ç¬¦
    123
    .
    4
    !
    56
    ~
    jerry
    ï¼
    æ•°
    å­¦
    ç¬¦
    å·
    ğŸ¤Œ
    ä¸­
    å›½
    ustc



```python
len(tokens)
```




    25



###  Rematchç®—æ³•
è§£å†³åŸæ–‡æœ¬contextä¸­answer_start_idï¼Œåˆ°tokenizeråŒ–ä»¥ååœ¨tokensåˆ—è¡¨ä¸­çš„indexæ˜ å°„ã€‚


```python
mapping = [[], [0], [1], [2]]
```


```python
    def rematch(text, tokens):
        """ç»™å‡ºåŸå§‹çš„textå’Œtokenizeåçš„tokensçš„æ˜ å°„å…³ç³»
        """
        text = text.lower()

        normalized_text, char_mapping = '', []
        for i, ch in enumerate(text):
            if True:
                ch = unicodedata.normalize('NFD', ch)
                ch = ''.join([c for c in ch if unicodedata.category(c) != 'Mn'])
            ch = ''.join([
                c for c in ch
                if not (ord(c) == 0 or ord(c) == 0xfffd or is_control(c))
            ])
            normalized_text += ch
            char_mapping.extend([i] * len(ch))

        text, token_mapping, offset = normalized_text, [], 0
        print(text)
        for token in tokens:
            start = text[offset:].index(token) + offset
            end = start + len(token)
            token_mapping.append(char_mapping[start:end])
            offset = end
        return token_mapping
```

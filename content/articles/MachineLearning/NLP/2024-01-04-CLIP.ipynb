{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425e610d-b585-4781-909a-4f956a000a2a",
   "metadata": {},
   "source": [
    "date: 2024-01-04 11:17:17\n",
    "author: Jerry Su\n",
    "slug: CLIP\n",
    "title: CLIP\n",
    "category: \n",
    "tags: LLM, NLP\n",
    "toc: show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93696114-0f30-4fb2-b9f5-5435ddb7e11f",
   "metadata": {},
   "source": [
    "### CLIP（Contrastive Language-Image Pretraining）\n",
    "\n",
    "**Vision Vocabulary**：CLIP的视觉字典是指该模型通过对比学习从大规模图像和文本数据中学到的关于图像的表示和语义信息的集合。\n",
    "在训练过程中，CLIP学会了将图像嵌入（embed）到一个高维空间中，并在该空间中通过文本描述对图像进行分类或检索。这个视觉字典的结构是通过模型的架构和训练数据来定义的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d9765-c92d-4d56-8659-b4bb9bd6a5a8",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c85d0-2212-416c-9df6-84fdaaa7f4d8",
   "metadata": {},
   "source": [
    "### 1.1 image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870241d4-ee9b-4421-9421-d79bebeedcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.The first processor, usually is the clip pretrained model (vit)\n",
    "# 224*224\n",
    "# image_processor do not used in opt\n",
    "from transformers.models.clip.image_processing_clip import CLIPImageProcessor.preprocess\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"~/.cache/huggingface/hub/models-openai-clip-vit-large-patch14\")\n",
    "processor = image_processor          \n",
    "image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "\n",
    "# 2.The second processor_high, usually is the designed image encoder (sam/swin/cnn)\n",
    "# 1024*1024\n",
    "from vary.model.plug.transforms import train_transform\n",
    "image_processor_high = train_transform\n",
    "processor_high = image_processor_high\n",
    "image_high = processor_high(image_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a589e41-56b9-4927-8fb0-00f2bc40094d",
   "metadata": {},
   "source": [
    "### 1.2 multimodal_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df5148-56fb-468e-829b-c6a3af5358fd",
   "metadata": {},
   "source": [
    "```\n",
    "<image> -> <img> + [<imgpad>] * 256 + </img>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2cb7d-544e-4e1f-bdb0-b960b9bc9584",
   "metadata": {},
   "source": [
    "### 1.3 token_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5da22b-faae-4d4e-8fc0-6bd4fb45c582",
   "metadata": {},
   "source": [
    "```\n",
    "    \"\"\"\n",
    "    Given a list of sources, each is a conversation list. This transform:\n",
    "    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n",
    "    2. Concatenate conversations together;\n",
    "    3. Tokenize the concatenated conversation;\n",
    "    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n",
    "    \"\"\"\n",
    "    DEFAULT_IM_START_TOKEN = <img>\n",
    "    DEFAULT_IM_END_TOKEN = </img>\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            'from': 'human', \n",
    "            'value': '<img><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad></img>'\n",
    "        }, \n",
    "        {\n",
    "            'from': 'gpt', \n",
    "            'value': 'The man is sitting on top of piled objects or belongings loaded into the back of a pickup truck.'}, {'from': 'human', 'value': 'Is the man holding anything in his hands?'\n",
    "        }, \n",
    "        {\n",
    "            'from': 'gpt', \n",
    "            'value': 'Yes, the man is holding a beer in his hand while sitting on top of the objects in the back of the pickup truck.'}, {'from': 'human', 'value': 'What color is the pickup truck?'\n",
    "        }, \n",
    "        {\n",
    "            'from': 'gpt', \n",
    "            'value': 'The pickup truck is white.'\n",
    "        }, \n",
    "        {\n",
    "            'from': 'human', \n",
    "            'value': 'Is the man sitting or standing?'\n",
    "        }, \n",
    "        {\n",
    "            'from': 'gpt', \n",
    "            'value': 'The man is sitting on top of the piled objects in the back of the pickup truck.'\n",
    "        }, \n",
    "        {\n",
    "            'from': 'human', \n",
    "            'value': 'What could be the possible reasons for the man sitting on top of the possessions in the back of the pickup truck?'\n",
    "        }, \n",
    "        {\n",
    "            'from': 'gpt', \n",
    "            'value': \"There could be several reasons for the man sitting on top of his possessions in the back of the pickup truck:\\n\\n1. Moving: The man might be moving to a new location and needed to transport his items in a pickup truck, utilizing available space efficiently. By sitting on top of the belongings, he could be helping to stabilize and secure the items during the move, preventing them from falling or shifting during transportation.\\n\\n2. Lack of seating: If the cab of the pickup truck is already at full capacity or there isn't enough space for him to sit inside, the man may have chosen to sit on his possessions as an alternative seating arrangement.\\n\\n3. Road trip or outing: The man might be on a road trip or a casual outing with friends or family, where he is using the back of the pickup truck as an open-air seating area. By sitting on top of the loaded items, he may be enjoying the journey while savoring his beer.\\n\\n4. Keeping an eye on belongings: The man could be safeguarding his possessions by staying close to them, ensuring that no items are lost, stolen or damaged during the journey.\\n\\nRegardless of the specific reason, the image shows a person making the most of their situation, adding a touch of lightheartedness or adventure to an otherwise mundane scene.\"\n",
    "        }\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bec286-29fa-43f2-afa7-ea93c92013d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\\n\".join(conversation[\"value\"]) + \"</s>\"    # # pseudo code\n",
    "\n",
    "tokenized_list = [\n",
    "    self.tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=self.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ) for text in strings\n",
    "]\n",
    "\n",
    "# input_ids = labels \n",
    "input_ids = labels = [\n",
    "    tokenized.input_ids[0] for tokenized in tokenized_list\n",
    "]\n",
    "\n",
    "# labels.repalce(\"...\", -100)\n",
    "IGNORE_INDEX = -100\n",
    "lables = labels.replace(\"<imgpad>\", -100).replace(\"human\", -100)  # pseudo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8cfe89-ffc3-4fb3-b568-cbd4ed66397a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09fc151c-af4a-477d-85ae-deea63f32f7c",
   "metadata": {},
   "source": [
    "## 2.Model\n",
    "\n",
    "### varyOPTModel -> OPTModel -> OPTDecoder -> OPTDecoderLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa424efc-888c-4f9c-9bb7-e6994429d5d6",
   "metadata": {},
   "source": [
    "### 2.1 varyOPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff81948-a680-4a7a-90d5-8f7dd3193727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class varyOPTModel(OPTModel):\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super(varyOPTModel, self).__init__(config)\n",
    "\n",
    "        self.vision_tower = build_sam_vit_b()\n",
    "\n",
    "        self.mm_projector = nn.Linear(1024, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6315dc7-3c1c-4fe5-a002-ad3a4980aebb",
   "metadata": {},
   "source": [
    "### 2.2 OPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0788d-1cae-4ab1-beeb-fde69a59bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTModel(OPTPreTrainedModel):\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__(config)\n",
    "        self.decoder = OPTDecoder(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda0c80-d860-4fa0-bb10-3727c06192d6",
   "metadata": {},
   "source": [
    "### 2.3 OPTDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a5ab3-fcf7-4e79-bf54-3ab7dfe3ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTDecoder(OPTPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`OPTDecoderLayer`]\n",
    "    \"\"\"\n",
    "    self.embed_tokens = nn.Embedding(config.vocab_size, config.word_embed_proj_dim, self.padding_idx)\n",
    "    self.embed_positions = OPTLearnedPositionalEmbedding(config.max_position_embeddings, config.hidden_size)\n",
    "    self.layers = nn.ModuleList([OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da82ce-1bc0-4aa6-851b-d260ac8a50e8",
   "metadata": {},
   "source": [
    "### 2.4 OPTDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fc87c-ad97-4506-983c-e290f5dcc384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTDecoderLayer(nn.Module):\n",
    "    self.self_attn = OPTAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.num_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            is_decoder=True,\n",
    "            bias=config.enable_bias,\n",
    "    )\n",
    "    self.activation_fn = ACT2FN[config.activation_function]\n",
    "\n",
    "    self.self_attn_layer_norm = nn.LayerNorm(\n",
    "        self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine\n",
    "    )\n",
    "    self.fc1 = nn.Linear(self.embed_dim, config.ffn_dim, bias=config.enable_bias)\n",
    "    self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=config.enable_bias)\n",
    "    self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d192bef-f8a6-4888-83fb-735dd320d5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ac847-54a2-4d4e-8413-e812b43a638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision tower build as below!!!\n",
    "self.vision_tower = build_sam_vit_b()\n",
    "\n",
    "def build_sam_vit_b(checkpoint=None):\n",
    "    return _build_sam(\n",
    "        encoder_embed_dim=768,\n",
    "        encoder_depth=12,\n",
    "        encoder_num_heads=12,\n",
    "        encoder_global_attn_indexes=[2, 5, 8, 11],\n",
    "        checkpoint=checkpoint,\n",
    "    )\n",
    "\n",
    "\n",
    "def _build_sam(\n",
    "    encoder_embed_dim,\n",
    "    encoder_depth,\n",
    "    encoder_num_heads,\n",
    "    encoder_global_attn_indexes,\n",
    "):\n",
    "    prompt_embed_dim = 256\n",
    "    image_size = 1024\n",
    "    vit_patch_size = 16\n",
    "    \n",
    "    # This class of ImageEncoderViT and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa\n",
    "\n",
    "    image_encoder=ImageEncoderViT(\n",
    "            depth=encoder_depth,\n",
    "            embed_dim=encoder_embed_dim,\n",
    "            img_size=image_size,\n",
    "            mlp_ratio=4,\n",
    "            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "            num_heads=encoder_num_heads,\n",
    "            patch_size=vit_patch_size,\n",
    "            qkv_bias=True,\n",
    "            use_rel_pos=True,\n",
    "            global_attn_indexes=encoder_global_attn_indexes,\n",
    "            window_size=14,\n",
    "            out_chans=prompt_embed_dim,\n",
    "        )\n",
    "\n",
    "    return image_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866f2b0-405d-4469-ba44-ec23f698f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image embed + text embed\n",
    "input_embeds = torch.cat(\n",
    "                        (\n",
    "                            input_embeds[:image_start_token_pos+1], \n",
    "                            image_features,    num_patches: 256\n",
    "                            input_embeds[image_start_token_pos + num_patches + 1:]\n",
    "                        ), \n",
    "                        dim=0\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d39e26-e047-4d1e-9d81-21e714e9f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n",
      "[NbConvertApp] Converting notebook 2024-01-04-Vary.ipynb to markdown\n",
      "[NbConvertApp] Writing 10547 bytes to 2024-01-04-Vary.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to markdown 2024-01-04-Vary.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0b5db-c993-4787-97ea-935d21882d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

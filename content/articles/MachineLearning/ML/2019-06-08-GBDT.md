Status: published
Date: 2019-06-08 02:18:10
Author: Jerry Su
Slug: GBDT
Title: GBDT
Category: 
Tags: Machine Learning, GBDT
summary: Reason is the light and the light of life.
toc: show

### 问题引入

给定数据集$(x_1, y_1), (x_2, y_2),...,(x_n, y_n)$, 拟合一个模型$F(x)$。

$F(x_1) = 1.4$而$y_1=1.3$

$F(x_2) = 0.9$而$y_2=0.8$

...

在不改变$F(x)$模型前提下，如何提升模型$F(x)$？

在原有模型$F(x)$基础上增加模型$h(x)$,即$F(x) + h(x)$。

$$F(x_n) + h(x_n) = y_n$$

$$h(x_n) = y_n - F(x_n)$$

所以对数据$(x_1, y_1-F(x_1)), (x_2, y_2-F(x_2)),...,(x_n, y_n-F(x_n))$拟合一个回归树$h$。

$y_i - F(x_i)$是**残差**，就是模型$F(x)$未很好拟合的那一部分误差。$h(x)$作用就是弥补现有模型的残差。如果$F+h$依然没有很好的拟合原数据，则继续添加另一个回归树模型去拟合新的残差。

### 残差与梯度下降的关系

梯度下降是指在函数当前点对应梯度的反方向迭代移动以达到函数的局部最小。

损失函数：$L(y, F(x)) = \frac{1}{2} (y - F(x))^2$

我们通过迭代调整$F(x_1), F(x_2),..,F(x_n)$以到达损失函数$J = \sum_i L(y_i, F(x_i))$的最小化。

注：$F(x_1), F(x_2),...,F(x_n)$仅仅是一些数，所以可以将$F(x_i)$当作是变量。那么梯度推导：

\begin{align}
\frac{\partial J}{\partial F(x_i)} & = \frac{\partial \sum_i L(y_i, F(x_i))}{\partial F(x_i)} \\ 
& = \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \\
& = \frac{\partial \frac{1}{2}(y_i-F(x_i))^2}{\partial x} \\
& = F(x_i)-y_i
\end{align}

所以对于具有平方损失的回归：

\begin{align}
残差y_i-F(x_i) &<===>负梯度-\frac{\partial J}{\partial F(x_i)} \\
对残差拟合h &<===> 对负梯度拟合h \\
基于残差更新模型F &<===> 基于负梯度更新F \\
\end{align}
所以使用梯度下降来更新模型F，梯度下降的概念比残差更通用且有用。

**残差与负梯度的关系：是由平方损失函数建立的，平方损失函数的一阶导数即梯度正是负残差，而平方损失函数又常用于回归问题**
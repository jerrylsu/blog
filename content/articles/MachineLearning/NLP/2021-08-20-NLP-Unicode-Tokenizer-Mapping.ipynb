{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date: 2021-08-20 10:17:17\n",
    "author: Jerry Su\n",
    "slug: NLP-Unicode-Tokenizer-Mapping\n",
    "title: NLP Unicode Tokenizer Mapping\n",
    "category: \n",
    "tags: Python, NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils.py\n",
    "# tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1.0'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.unidata_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Po\n",
      "Pd\n",
      "Po\n",
      "Zs\n",
      "Ll\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n",
      "Mn\n"
     ]
    }
   ],
   "source": [
    "# unicodeç±»å‹æšä¸¾\n",
    "# https://www.fileformat.info/info/unicode/category/index.htm\n",
    "\n",
    "print(unicodedata.category('.'))\n",
    "print(unicodedata.category('-'))\n",
    "print(unicodedata.category(','))\n",
    "print(unicodedata.category(' '))\n",
    "#unicodedata.category('aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹')\n",
    "for ch in 'aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹':\n",
    "    print(unicodedata.category(ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "10\n",
      "13\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# http://c.biancheng.net/c/ascii/\n",
    "# ord()è¿”å›åè¿›åˆ¶\n",
    "print(ord(' '))\n",
    "print(ord('\\n'))\n",
    "print(ord('\\r'))\n",
    "print(ord('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# åˆ¤æ–­ç©ºæ ¼\n",
    "def is_space(ch):\n",
    "    \"\"\"ç©ºæ ¼ç±»å­—ç¬¦åˆ¤æ–­åˆ¤æ–­ã€‚\n",
    "    \n",
    "    ç©ºæ ¼å­—ç¬¦åŒ…æ‹¬ï¼š' ', '\\n', '\\t', 'r'\n",
    "    \"\"\"\n",
    "    return ch == ' ' or \\\n",
    "           ch == '\\t' or \\\n",
    "           ch == '\\r' or \\\n",
    "           ch == '\\n' or \\\n",
    "           unicodedata.category(ch) == 'Zs'  # [Zs] Separator, Space\n",
    "\n",
    "print(is_space(' '))\n",
    "print(is_space('\\n'))\n",
    "print(is_space('\\r'))\n",
    "print(is_space('\\t'))\n",
    "print(is_space('A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sm'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åˆ¤æ–­æ ‡ç‚¹ç¬¦å·\n",
    "def is_punctuation(ch):\n",
    "    \"\"\"æ ‡ç‚¹ç¬¦å·ç±»å­—ç¬¦åˆ¤æ–­ï¼ˆåŒ…å«å…¨/åŠè§’ï¼‰ã€‚\n",
    "    è‹±æ–‡æ ‡ç‚¹ç¬¦å·æ˜¯åŠè§’ï¼Œä¸­æ–‡æ ‡ç‚¹ç¬¦å·æ˜¯å…¨è§’ã€‚å…¨è§’å ä¸€ä¸ªå­—ç¬¦ï¼ŒåŠè§’å ä¸¤ä¸ªå­—ç¬¦ã€‚\n",
    "    \n",
    "    [33, 47]    ! \" # $ % & ' ( ) * + , - . /\n",
    "    [58, 64]    : ; < = > ? @\n",
    "    [91, 96]    [ \\ ] ^ _ `\n",
    "    [123, 126]  { | } ~\n",
    "    \n",
    "    unicodedata.category.(ch).startswith('P')\n",
    "    # https://www.fileformat.info/info/unicode/category/index.htm]\n",
    "    # åŒ…å«äº†æ‰€æœ‰æ ‡ç‚¹ç¬¦å·[Pc][Pd][Pe][Pf][Pi][Po][Ps]\n",
    "    \"\"\"\n",
    "    code = ord(ch)\n",
    "    return 33 <= code <= 47 or \\\n",
    "           58 <= code <= 64 or \\\n",
    "           91 <= code <= 96 or \\\n",
    "           123 <= code <= 126 or \\\n",
    "           unicodedata.category(ch).startswith('P')\n",
    "is_punctuation('ï¼Ÿ') # ä¸­æ–‡ï¼Ÿå·\n",
    "unicodedata.category('âˆ«')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_control(ch):\n",
    "    \"\"\"æ§åˆ¶ç±»å­—ç¬¦åˆ¤æ–­\n",
    "    https://en.wikipedia.org/wiki/Control_character\n",
    "    https://www.fileformat.info/info/unicode/category/Cc/index.htm\n",
    "    https://www.fileformat.info/info/unicode/category/Cf/index.htm\n",
    "    \n",
    "    \"\"\"\n",
    "    return unicodedata.category(ch) in ('Cc', 'Cf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cjk_character(ch):\n",
    "    \"\"\"CJKç±»å­—ç¬¦åˆ¤æ–­ï¼ˆåŒ…æ‹¬ä¸­æ–‡å­—ç¬¦ä¹Ÿåœ¨æ­¤åˆ—ï¼‰\n",
    "    å‚è€ƒï¼šhttps://en.wikipedia.org/wiki/Unicode_block\n",
    "    \n",
    "    # CJK Unified Ideographs, HAN\n",
    "    # CJK Unified Ideographs Extension A, HAN\n",
    "    # General Punctuation\n",
    "    # Supplemental Mathematical Operators\n",
    "    # Miscellaneous Symbols and Arrows\n",
    "    # Miscellaneous Symbols and Arrows\n",
    "    # CJK Compatibility Ideographs, HAN\n",
    "    # CJK Compatibility Ideographs Supplement, HAN\n",
    "    \"\"\"\n",
    "    code = ord(ch)\n",
    "    return 0x4E00 <= code <= 0x9FFF or \\\n",
    "           0x3400 <= code <= 0x4DBF or \\\n",
    "           0x20000 <= code <= 0x2A6DF or \\\n",
    "           0x2A700 <= code <= 0x2B73F or \\\n",
    "           0x2B740 <= code <= 0x2B81F or \\\n",
    "           0x2B820 <= code <= 0x2CEAF or \\\n",
    "           0xF900 <= code <= 0xFAFF or \\\n",
    "           0x2F800 <= code <= 0x2FA1F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin: aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹bæ˜¯ç‰¹æ®Šmnå­—ç¬¦123.4! 56~ jerryï¼ æ•°å­¦ç¬¦å·ğŸ¤Œä¸­å›½ustc : 57\n",
      "Normalize: aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹bæ˜¯ç‰¹æ®Šmnå­—ç¬¦123.4! 56~ jerryï¼ æ•°å­¦ç¬¦å·ğŸ¤Œä¸­å›½ustc : 57\n",
      "Text: abæ˜¯ç‰¹æ®Šmnå­—ç¬¦123.4! 56~ jerryï¼ æ•°å­¦ç¬¦å·ğŸ¤Œä¸­å›½ustc : 38\n",
      "Spaced text: ab æ˜¯  ç‰¹  æ®Š mn å­—  ç¬¦ 123 . 4 !  56 ~  jerry ï¼   æ•°  å­¦  ç¬¦  å· ğŸ¤Œ ä¸­  å›½ ustc\n"
     ]
    }
   ],
   "source": [
    "\"\"\"åˆ†è¯å™¨\n",
    "\"\"\"\n",
    "\n",
    "origin_text = \"aà¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹à¹bæ˜¯ç‰¹æ®ŠMnå­—ç¬¦123.4! 56~ jerryï¼ æ•°å­¦ç¬¦å·ğŸ¤Œä¸­å›½USTC\"\n",
    "text = origin_text.lower()\n",
    "print(f'Origin: {text} : {len(text)}')\n",
    "\n",
    "# ç”±äºå­˜åœ¨Mnå­—ç¬¦ç­‰ä¸€äº›ç‰¹æ®Šå­—ç¬¦ï¼Œå…ˆè§„èŒƒåŒ–normalizeï¼ŒNFDè§„èŒƒåˆ†è§£å½¢å¼\n",
    "text = unicodedata.normalize('NFD', text)\n",
    "print(f'Normalize: {text} : {len(text)}')\n",
    "\n",
    "# åˆ é™¤Mnå­—ç¬¦\n",
    "text = ''.join([ch for ch in text if unicodedata.category(ch) != 'Mn'])\n",
    "print(f'Text: {text} : {len(text)}')\n",
    "\n",
    "\n",
    "# ç©ºæ ¼åˆ†éš”æ–‡æœ¬\n",
    "spaced = ''\n",
    "for ch in text:\n",
    "    # æ ‡ç‚¹ç¬¦åˆå’Œcjkå­—ç¬¦ï¼Œå‰åç©ºæ ¼åˆ†éš”\n",
    "    if is_punctuation(ch) or is_cjk_character(ch):\n",
    "        spaced += ' ' + ch + ' '\n",
    "    # ç©ºæ ¼ï¼Œå³ç½®ç©ºæ ¼\n",
    "    elif is_space(ch):   \n",
    "        spaced += ' '\n",
    "    # åˆ é™¤0(NULL)ï¼Œ0xfffdï¼Œæ§åˆ¶å­—ç¬¦ï¼Œè¿™äº›å­—ç¬¦å‡æ˜¯ä¸å¯è§ï¼Œæ— æ³•æ˜¾ç¤ºçš„\n",
    "    elif ord(ch) == 0 or ord(ch) == 0xfffd or is_control(ch):  \n",
    "        continue\n",
    "    # æ•°å­—/è‹±æ–‡å­—æ¯/æ•°å­¦ç¬¦å·ï¼Œç›´æ¥æ‹¼æ¥\n",
    "    else:    \n",
    "        spaced += ch\n",
    "print(f'Spaced text: {spaced}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-23 02:00:28--  https://huggingface.co/hfl/chinese-roberta-wwm-ext/blob/main/vocab.txt\n",
      "Resolving huggingface.co (huggingface.co)... 34.200.164.230, 54.84.221.171, 34.195.144.223, ...\n",
      "Connecting to huggingface.co (huggingface.co)|34.200.164.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4564022 (4.4M) [text/html]\n",
      "Saving to: â€˜vocab.txtâ€™\n",
      "\n",
      "vocab.txt           100%[===================>]   4.35M  2.58MB/s    in 1.7s    \n",
      "\n",
      "2021-08-23 02:00:32 (2.58 MB/s) - â€˜vocab.txtâ€™ saved [4564022/4564022]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/hfl/chinese-roberta-wwm-ext/blob/main/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path):\n",
    "    tokens_dict = {}\n",
    "    with open(vocab_path, encoding='utf-8') as fp:\n",
    "        for line in fp:\n",
    "            token = line.split()\n",
    "            token = token[0] if token else line.strip()\n",
    "            tokens_dict[token] = len(tokens_dict)\n",
    "    return tokens_dict\n",
    "tokens_dict = load_vocab('../../../cache/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_piece_tokenize(word):\n",
    "    \"\"\"wordå†…åˆ†æˆsubword, _word_maxlen=200\n",
    "    \"\"\"\n",
    "    if len(word) > 200:\n",
    "        return [word]\n",
    "\n",
    "    tokens, start, end = [], 0, 0\n",
    "    while start < len(word):\n",
    "        end = len(word)\n",
    "        while end > start:\n",
    "            sub = word[start:end]\n",
    "            if start > 0:\n",
    "                sub = '##' + sub\n",
    "            if sub in tokens_dict:\n",
    "                break\n",
    "            end -= 1\n",
    "        if start == end:\n",
    "            return [word]\n",
    "        else:\n",
    "            tokens.append(sub)\n",
    "            start = end\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\n",
      "æ˜¯\n",
      "ç‰¹\n",
      "æ®Š\n",
      "mn\n",
      "å­—\n",
      "ç¬¦\n",
      "123\n",
      ".\n",
      "4\n",
      "!\n",
      "56\n",
      "~\n",
      "jerry\n",
      "ï¼\n",
      "æ•°\n",
      "å­¦\n",
      "ç¬¦\n",
      "å·\n",
      "ğŸ¤Œ\n",
      "ä¸­\n",
      "å›½\n",
      "ustc\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for word in spaced.strip().split():\n",
    "    print(word)\n",
    "    tokens.extend(word_piece_tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Rematchç®—æ³•\n",
    "è§£å†³åŸæ–‡æœ¬contextä¸­answer_start_idï¼Œåˆ°tokenizeråŒ–ä»¥ååœ¨tokensåˆ—è¡¨ä¸­çš„indexæ˜ å°„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = [[], [0], [1], [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def rematch(text, tokens):\n",
    "        \"\"\"ç»™å‡ºåŸå§‹çš„textå’Œtokenizeåçš„tokensçš„æ˜ å°„å…³ç³»\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "\n",
    "        normalized_text, char_mapping = '', []\n",
    "        for i, ch in enumerate(text):\n",
    "            if True:\n",
    "                ch = unicodedata.normalize('NFD', ch)\n",
    "                ch = ''.join([c for c in ch if unicodedata.category(c) != 'Mn'])\n",
    "            ch = ''.join([\n",
    "                c for c in ch\n",
    "                if not (ord(c) == 0 or ord(c) == 0xfffd or is_control(c))\n",
    "            ])\n",
    "            normalized_text += ch\n",
    "            char_mapping.extend([i] * len(ch))\n",
    "\n",
    "        text, token_mapping, offset = normalized_text, [], 0\n",
    "        print(text)\n",
    "        for token in tokens:\n",
    "            start = text[offset:].index(token) + offset\n",
    "            end = start + len(token)\n",
    "            token_mapping.append(char_mapping[start:end])\n",
    "            offset = end\n",
    "        return token_mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:blog] *",
   "language": "python",
   "name": "conda-env-blog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

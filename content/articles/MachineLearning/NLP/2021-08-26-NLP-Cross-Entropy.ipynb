{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date: 2021-08-26 10:17:17\n",
    "author: Jerry Su\n",
    "slug: Cross-Entropy\n",
    "title: Cross Entropy\n",
    "category: \n",
    "tags: NLP\n",
    "summary: Reason is the light and the light of life.\n",
    "toc: show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解几个概念：\n",
    "\n",
    "[必看：交叉熵如何做损失函数？](https://www.bilibili.com/video/BV15V411W7VB?from=search&seid=11935226210603570930)\n",
    "\n",
    "- 信息\n",
    "\n",
    "- 信息量\n",
    "\n",
    "$H\\left( X\\right) =-\\log P\\left( X\\right)$\n",
    "\n",
    "- 熵\n",
    "\n",
    "$Entropy\\left( X\\right) =-\\sum ^{i}_{n=1}p\\left( x_{i}\\right) \\log p\\left( x_{i}\\right)$\n",
    "\n",
    "系统熵，就是一个概率系统信息量的期望，即**所有可能发生的事件的信息量**乘以**事件发生的概率**后**求和**。\n",
    "\n",
    "- 相对熵（KL散度）\n",
    "\n",
    "两个概率系统\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "\n",
    "- 交叉熵\n",
    "\n",
    "http://www.jerrylsu.net/articles/2018/ml-Information-Theory.html\n",
    "\n",
    "https://blog.csdn.net/tsyccnh/article/details/79163834"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:blog] *",
   "language": "python",
   "name": "conda-env-blog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
